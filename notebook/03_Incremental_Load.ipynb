{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "817a47b9-3420-4148-9342-462fede679a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Secrets loaded from Key Vault\n✅ Base path: abfss://healthcare-project@adls31tejashree.dfs.core.windows.net\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CELL 1: Setup & Key Vault Secrets\n",
    "# ============================================\n",
    "storage_account_name = \"adls31tejashree\"\n",
    "container_name       = \"healthcare-project\"\n",
    "\n",
    "# Fetch secrets securely from Key Vault\n",
    "storage_account_key = dbutils.secrets.get(\n",
    "    scope=\"healthcare-kv-scope\",\n",
    "    key=\"adls-account-key\"\n",
    ")\n",
    "\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\",\n",
    "    storage_account_key\n",
    ")\n",
    "\n",
    "base_path = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net\"\n",
    "watermark_path = f\"{base_path}/silver/watermark/\"\n",
    "\n",
    "print(f\"✅ Secrets loaded from Key Vault\")\n",
    "print(f\"✅ Base path: {base_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e06483b-b357-4f3e-ae2f-d08814c90656",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Old watermark deleted!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# RESET WATERMARK (Run once to reset)\n",
    "# ============================================\n",
    "try:\n",
    "    dbutils.fs.rm(watermark_path, recurse=True)\n",
    "    print(\"✅ Old watermark deleted!\")\n",
    "except:\n",
    "    print(\"⚠️ No watermark to delete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d2d3a09-99d9-4923-b5e3-f3d505a23c71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Existing watermark found: 2024-01-01 00:00:00\n\uD83D\uDCC5 Processing records after: 2024-01-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CELL 2: Initialize Watermark Table\n",
    "# (Only runs once on first execution)\n",
    "# ============================================\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "from pyspark.sql.functions import lit\n",
    "from datetime import datetime\n",
    "\n",
    "def initialize_watermark():\n",
    "    try:\n",
    "        df_watermark = spark.read.parquet(watermark_path)\n",
    "        last_watermark = df_watermark.collect()[0][\"last_processed_date\"]\n",
    "        print(f\"✅ Existing watermark found: {last_watermark}\")\n",
    "        return last_watermark\n",
    "    except:\n",
    "        # First run — set watermark to very old date to process all records\n",
    "        print(\"⚠️ No watermark found — initializing for first run\")\n",
    "        schema = StructType([\n",
    "            StructField(\"table_name\",          StringType(),    True),\n",
    "            StructField(\"last_processed_date\", TimestampType(), True)\n",
    "        ])\n",
    "        df_init = spark.createDataFrame(\n",
    "            [(\"hospital_records\", datetime(2024, 1, 1))],\n",
    "            schema\n",
    "        )\n",
    "        df_init.write.mode(\"overwrite\").parquet(watermark_path)\n",
    "        return datetime(2024, 1, 1)\n",
    "\n",
    "last_watermark = initialize_watermark()\n",
    "print(f\"\uD83D\uDCC5 Processing records after: {last_watermark}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ae3a620-97af-4389-9233-a22193a3cfb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCCA Total records in Bronze:  55500\n✅ New records to process:   3823\n⏭️  Skipped (already processed): 51677\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CELL 3: Read Bronze & Filter New Records Only\n",
    "# ============================================\n",
    "from pyspark.sql.functions import col, to_timestamp, lit\n",
    "\n",
    "bronze_path = f\"{base_path}/bronze/hospital_records/healthcare_dataset.csv\"\n",
    "\n",
    "df_bronze = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(bronze_path)\n",
    "\n",
    "# Parse admission date AND drop original to avoid ambiguity\n",
    "df_bronze = df_bronze \\\n",
    "    .withColumn(\n",
    "        \"Date_of_Admission\",\n",
    "        to_timestamp(col(\"Date of Admission\"), \"yyyy-MM-dd\")\n",
    "    ) \\\n",
    "    .drop(\"Date of Admission\")  # ← drop original column with spaces\n",
    "\n",
    "# Filter only NEW records since last watermark\n",
    "df_incremental = df_bronze.filter(\n",
    "    col(\"Date_of_Admission\") > lit(last_watermark)\n",
    ")\n",
    "\n",
    "total_records = df_bronze.count()\n",
    "new_records   = df_incremental.count()\n",
    "\n",
    "print(f\"\uD83D\uDCCA Total records in Bronze:  {total_records}\")\n",
    "print(f\"✅ New records to process:   {new_records}\")\n",
    "print(f\"⏭️  Skipped (already processed): {total_records - new_records}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79d84f2a-842e-4514-8c22-dcfd39fdac77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Transformed 3788 new records\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CELL 4: Transform New Records\n",
    "# ============================================\n",
    "from pyspark.sql.functions import (\n",
    "    datediff, round, upper, trim, when,\n",
    "    to_date, year, col\n",
    ")\n",
    "\n",
    "if new_records == 0:\n",
    "    print(\"⚠️ No new records to process — pipeline complete!\")\n",
    "    dbutils.notebook.exit(\"No new records\")\n",
    "\n",
    "# Clean column names (spaces → underscores)\n",
    "df_clean = df_incremental \\\n",
    "    .withColumnRenamed(\"Date of Admission\",  \"Date_of_Admission\") \\\n",
    "    .withColumnRenamed(\"Blood Type\",         \"Blood_Type\") \\\n",
    "    .withColumnRenamed(\"Medical Condition\",  \"Medical_Condition\") \\\n",
    "    .withColumnRenamed(\"Insurance Provider\", \"Insurance_Provider\") \\\n",
    "    .withColumnRenamed(\"Billing Amount\",     \"Billing_Amount\") \\\n",
    "    .withColumnRenamed(\"Room Number\",        \"Room_Number\") \\\n",
    "    .withColumnRenamed(\"Admission Type\",     \"Admission_Type\") \\\n",
    "    .withColumnRenamed(\"Discharge Date\",     \"Discharge_Date\") \\\n",
    "    .withColumnRenamed(\"Test Results\",       \"Test_Results\")\n",
    "\n",
    "# Apply transformations\n",
    "df_clean = df_clean \\\n",
    "    .withColumn(\"Date_of_Admission\", to_date(col(\"Date_of_Admission\"))) \\\n",
    "    .withColumn(\"Discharge_Date\",    to_date(col(\"Discharge_Date\"), \"yyyy-MM-dd\")) \\\n",
    "    .withColumn(\"Length_of_Stay\",    datediff(col(\"Discharge_Date\"), col(\"Date_of_Admission\"))) \\\n",
    "    .withColumn(\"Billing_Amount\",    round(col(\"Billing_Amount\"), 2)) \\\n",
    "    .withColumn(\"Gender\",            upper(trim(col(\"Gender\")))) \\\n",
    "    .withColumn(\"Medical_Condition\", upper(trim(col(\"Medical_Condition\")))) \\\n",
    "    .withColumn(\"Admission_Type\",    upper(trim(col(\"Admission_Type\")))) \\\n",
    "    .withColumn(\"Test_Results\",      upper(trim(col(\"Test_Results\")))) \\\n",
    "    .dropDuplicates() \\\n",
    "    .filter(col(\"Billing_Amount\") > 0) \\\n",
    "    .filter(col(\"Age\").isNotNull())\n",
    "\n",
    "print(f\"✅ Transformed {df_clean.count()} new records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46c0ae97-5e43-462b-88af-dfd6cd7fcc1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 3788 new records written to Silver\n\uD83D\uDCC1 Path: abfss://healthcare-project@adls31tejashree.dfs.core.windows.net/silver/hospital_records/processed_20260215/\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CELL 5: Write Incremental Data to Silver\n",
    "# (Append mode — preserves existing data!)\n",
    "# ============================================\n",
    "from datetime import datetime\n",
    "\n",
    "process_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "silver_output_path = f\"{base_path}/silver/hospital_records/processed_{process_date}/\"\n",
    "\n",
    "df_clean.write \\\n",
    "    .mode(\"append\") \\\n",
    "    .parquet(silver_output_path)\n",
    "\n",
    "print(f\"✅ {df_clean.count()} new records written to Silver\")\n",
    "print(f\"\uD83D\uDCC1 Path: {silver_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bcf7e80-a144-44a0-9161-9a4e6a7b3ecf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Gold: readmission_rates updated!\n✅ Gold: diagnosis_trends updated!\n✅ Gold: avg_treatment_costs updated!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CELL 6: Update Gold Tables with New Data\n",
    "# ============================================\n",
    "\n",
    "# Gold 1: Readmission Rates\n",
    "from pyspark.sql.functions import count, avg, col, round, when, year\n",
    "df_readmission = df_clean \\\n",
    "    .groupBy(\"Medical_Condition\", \"Admission_Type\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"Total_Admissions\"),\n",
    "        avg(\"Length_of_Stay\").alias(\"Avg_Length_of_Stay\"),\n",
    "        avg(\"Billing_Amount\").alias(\"Avg_Billing_Amount\")\n",
    "    ) \\\n",
    "    .withColumn(\"Avg_Length_of_Stay\", round(col(\"Avg_Length_of_Stay\"), 2)) \\\n",
    "    .withColumn(\"Avg_Billing_Amount\", round(col(\"Avg_Billing_Amount\"), 2))\n",
    "\n",
    "df_readmission.write.mode(\"append\").parquet(\n",
    "    f\"{base_path}/gold/hospital_records/readmission_rates/\"\n",
    ")\n",
    "print(\"✅ Gold: readmission_rates updated!\")\n",
    "\n",
    "# Gold 2: Diagnosis Trends\n",
    "df_diagnosis = df_clean \\\n",
    "    .withColumn(\"Age_Group\",\n",
    "        when(col(\"Age\") < 18,  \"0-17\")\n",
    "        .when(col(\"Age\") < 35, \"18-34\")\n",
    "        .when(col(\"Age\") < 50, \"35-49\")\n",
    "        .when(col(\"Age\") < 65, \"50-64\")\n",
    "        .otherwise(\"65+\")\n",
    "    ) \\\n",
    "    .withColumn(\"Admission_Year\", year(col(\"Date_of_Admission\"))) \\\n",
    "    .groupBy(\"Medical_Condition\", \"Age_Group\", \"Admission_Year\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"Case_Count\"),\n",
    "        avg(\"Billing_Amount\").alias(\"Avg_Cost\")\n",
    "    ) \\\n",
    "    .withColumn(\"Avg_Cost\", round(col(\"Avg_Cost\"), 2))\n",
    "\n",
    "df_diagnosis.write.mode(\"append\").parquet(\n",
    "    f\"{base_path}/gold/hospital_records/diagnosis_trends/\"\n",
    ")\n",
    "print(\"✅ Gold: diagnosis_trends updated!\")\n",
    "\n",
    "# Gold 3: Avg Treatment Costs\n",
    "df_costs = df_clean \\\n",
    "    .groupBy(\"Medical_Condition\", \"Medication\", \"Insurance_Provider\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"Patient_Count\"),\n",
    "        avg(\"Billing_Amount\").alias(\"Avg_Treatment_Cost\"),\n",
    "        avg(\"Length_of_Stay\").alias(\"Avg_Stay_Days\")\n",
    "    ) \\\n",
    "    .withColumn(\"Avg_Treatment_Cost\", round(col(\"Avg_Treatment_Cost\"), 2)) \\\n",
    "    .withColumn(\"Avg_Stay_Days\",      round(col(\"Avg_Stay_Days\"), 2))\n",
    "\n",
    "df_costs.write.mode(\"append\").parquet(\n",
    "    f\"{base_path}/gold/hospital_records/avg_treatment_costs/\"\n",
    ")\n",
    "print(\"✅ Gold: avg_treatment_costs updated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f13241a9-648b-4caa-a0c9-e41f82a4e3da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Watermark updated to: 2026-02-15 08:18:31.084453\n\uD83D\uDCC5 Max admission date processed: 2024-05-07\n\uD83C\uDF89 Incremental load complete!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CELL 7: Update Watermark for Next Run\n",
    "# ============================================\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "from datetime import datetime\n",
    "\n",
    "# Get max admission date from newly processed records\n",
    "max_date = df_clean.agg({\"Date_of_Admission\": \"max\"}).collect()[0][0]\n",
    "new_watermark = datetime.now()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"table_name\",          StringType(),    True),\n",
    "    StructField(\"last_processed_date\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "df_new_watermark = spark.createDataFrame(\n",
    "    [(\"hospital_records\", new_watermark)],\n",
    "    schema\n",
    ")\n",
    "\n",
    "df_new_watermark.write.mode(\"overwrite\").parquet(watermark_path)\n",
    "\n",
    "print(f\"✅ Watermark updated to: {new_watermark}\")\n",
    "print(f\"\uD83D\uDCC5 Max admission date processed: {max_date}\")\n",
    "print(f\"\uD83C\uDF89 Incremental load complete!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_Incremental_Load",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}